## GekkoFS 优化方案

### 数据

**（1）数据嵌入请求**

<img src="images\数据嵌入请求.png" alt="数据嵌入请求" style="zoom:67%;" />

**方案内容**：

对于 read 和 write 操作，其数据和请求是分开的，数据用 RDMA 传输，请求用 RPC 消息承载。当需要传输的数据量较少时，可考虑将数据嵌入 RPC 消息中一并发送。

具体来说，对于 write 操作，可将 write data 嵌入发往目标节点的写请求消息；对于 read 操作，可将 read data 嵌入返回客户端的应答消息。为了不增加原消息结构体的大小，可考虑新增一消息类型用来单独处理这类小 I/O 情况。在向每个目标节点发送请求时，若数据规模小于某一阈值，则换用该消息类型。合适的阈值可能需要通过实验来确定，大概率会小于 chunksize。

**适用场景**：

小规模文件I/O。可节省一次 RDMA。对 I/O size 的限制可能比较苛刻。



### 元数据

**（1）批量元数据变更**

**方案内容**：

客户端在本地缓存多个元数据操作，一次性打包成一个 RPC，发送给 daemon。可有效减少 RPC 次数，提高系统吞吐量。

具体来说，为每一个目标节点维护一个操作队列。客户端执行元数据变更时，先把变更加入相应的队列，而不是立即发送请求。

触发持久化的条件：

1.客户端需要读取该节点上的元数据时；

2.队列大小超过一定阈值；

3.客户端调用 fsync。

**适用场景**：

大量元数据变更操作；不存在多个进程对同一文件的共享。

**问题**：

多个进程在共享某一文件时，异步持久化可能导致数据的一致性问题。例如，当进程 A 访问某一文件的元数据时，若其他进程对于该元数据的修改尚未落盘，A 可能会读到过期的数据。

**可能并非是一个严重的问题。**

**1.GekkoFS 本身就可能出现这种问题（网络延迟导致），因此批量元数据变更引入的异步持久化可视为一种更加严重的网络延迟。**

**2.用户可通过fsync等同步系统调用来保证元数据变更及时落盘。**



### 数据和元数据

**（1）合并数据和元数据请求**

<img src="images\合并数据和元数据请求.png" alt="合并数据和元数据请求" style="zoom:80%;" />

**方案内容**：

write 操作需要更新文件大小并写入数据。元数据可以和 hash 到同一目标节点的数据共用一条 RPC 消息，客户端不必单独再向该节点发送一次写请求。

此优化不适用于 append 操作，因为在 append 操作中，文件大小的更新和数据的写入具有严格的先后关系。元数据节点在完成对 size 的更新后，会返回一个 offset ，用于帮助客户端确定数据写入的偏移。也就是说，在更新完成之前，客户端不知道数据应当被写入文件的哪个位置，更不可能确定哪些块会被发送到哪些节点，因此无法和元数据共用一条 RPC 消息。

truncate 操作需要更新文件大小并删除数据，过程同上。

GekkoFS 对 remove 操作已实现此优化（但感觉代码写的有问题）。

**适用场景**：

1.同时涉及数据和元数据变更的 I/O 操作，如 write、truncate。

2.不同的 chunk 可能会被转发到不同的节点处理，只有和元数据位于同一台机器的 chunk，其 RPC 消息能被合并到元数据。

**（2）chunk0 存入 RocksDB**

**方案内容**：

将文件的首个数据块直接存入 RocksDB 的 Value 部分，绕过节点本地文件系统。所有对 chunk0 的访问，都将被转换为 RocksDB 中的增删改查。

**适用场景**：

小文件写。好处：

1.避免为小文件分配独立数据块文件，减少节点本地文件系统的 inode 和目录项开销。

2.在（1）的基础上，进一步合并服务端的数据和元数据操作，例如，IncreaseSize 和 write chunk0 可合并为对 RocksDB 中同一个键值对的操作，I/O 次数由原来的两次变为一次。



后续可能的优化：

- 客户端缓存。
- 服务端缓存。
- 缩短 tasklet 的原子性。

