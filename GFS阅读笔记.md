# 1. 设计概要

## 1.1 假设

+ 组件故障是常态而非意外，这要求系统拥有持续监控、错误检测、容错和快速自动恢复等功能。
+ 系统存储少量的**大文件**，因此设计假设和参数如 I/O 操作和块大小需要重新调整以实现高效管理；系统支持小文件但不对其进行优化。
+ 写操作主要包括大型顺序追加和极少数随机写，且一旦写入，文件就很少被修改，且大多数读都是顺序的。**系统的设计应当考虑文件访问模式的特点**，对于追加操作，需要在保证并发操作的原子性的同时最小化同步开销；对于小型随机写入操作，系统应当支持但不必高效（ Amdahl 定律）；由于大多数访问是顺序的，局部性较弱，客户端缓存数据块的意义不大。
+ 高持续带宽比低延迟更重要。
+ 文件系统和应用程序**协同设计**以提升系统的灵活性。例如， GFS 采用了一种宽松的一致性模型，这会造成某些记录的重复出现和填充，应用程序应当能够处理这种情况。

##1.2 接口

​	GFS 没有实现像 POSIX 这样的标准 API ，其支持的文件操作包括创建、删除、打开、关闭、读、写以及快照和记录追加。其中**记录追加**是 GFS 所特有的，它能保证并发场景下追加操作的原子性，且无需加锁，提高了系统性能。

## 1.3 架构

![GFS架构](images\GFS架构.png)

​	GFS 是典型的**中心化架构**设计。 GFS 集群由一个主节点和多个块服务器组成，可被多个客户端访问，如图所示。每个客户端通常是一台运行用户级服务器进程的普通Linux机器。

​	**块服务器储存实际的文件数据**。文件被划分为64MB大小的块，每个块由一个64 bits 的句柄唯一标识。每个块在多个块服务器上存有副本，默认是3个。

​	**主服务器储存数据的元信息**。包括命名空间、访问控制信息、文件到块的映射关系、以及块副本当前所处的位置。主服务器根据这些信息进行定期扫描并统筹控制整个系统的活动，如进行块租约管理、垃圾回收和块的迁移平衡。此外，主服务器还定期通过 HeartBeat 消息与每个块服务器通信，以给出指令并收集其状态。

​	GFS 客户端实现了文件系统的 API 并以库函数的形式提供给应用程序使用。客户端代表应用与服务器通信以实现数据的读和写。

​	客户端和块服务器都不缓存文件数据。大部分应用流式读取大型文件或数据块太大，因此**客户端缓存优化的意义不大，而且取消缓存可以避免同步问题，简化系统设计**。（但客户端会缓存元数据）。块服务器则可以利用 linux 自带的 buffer cache 提供的缓存功能。

##1.4 单点 Master

​	**优点：简化系统设计，便于主服务器利用全局信息实现复杂的全局控制策略。缺点：单个master带宽有限，容易成为性能瓶颈，且没有容错能力**

​	读取流程：

+ 客户端根据应用程序指定的文件名和偏移量计算得到对应的块索引，然后向 master 发送一个包含文件名和块索引的读取请求。 master 返回对应的块句柄和副本所在的位置，客户端将该映射关系缓存。
+ 客户端选择一个最近的副本发送访问请求，指明块句柄和字节范围。

​	客户和 master 仅交换元数据，而缓存机制进一步减少了客户端和 master 之间重复数据的交互。还可以更进一步优化，如客户端一次性请求多个数据块、 master 额外返回紧随其后数据块的信息（类似于**数据预取**），这些都有助于减少未来主客交互的次数。

## 1.5 块大小

​	选择了 64MB 这样一个较大的尺寸作为块大小，其优势在于：

+ **减少客户端向 master 请求块信息的次数**。访问一个块只需要向 master 初始请求一次信息。数据量一定，块越大，块数目越少，请求次数也越少。
+ **有利于将各个访问集中于一个块**，可过持续 TCP 连接减少网络通信开销。
+ **有利于减少 master 上存储的元数据**，因此元数据可以放入内存，带来一系列好处。

​	劣势在于：

+ 更大的块大小会导致更多的内部碎片和空间浪费，可通过**延迟空间分配策略**来缓解。
+ 如果多个客户端同时访问一个较小的文件，且该文件的块集中在一个服务器上，该服务器可能会成为热点。但这并不是一个主要问题，因为我们的应用程序以顺序读取大型文件为主。解决方法包括增加副本个数、错开应用程序的启动时间和从其他客户端读取数据。

## 1.6 元数据

​	master 在**内存**中存放三种类型的元数据：**1.文件和数据块的命名空间。2.文件到块的映射关系。3.每一个块的副本位置**。1和2还通过操作日志实现持久化存储，日志存放在本地磁盘并被复制到远程机器上。3不会被持久存储，而是由 master 通过询问块服务器获得。

### 1.6.1 内存数据结构

​	由于元数据保存在内存中， master相关操作效率很高。 master 可通过定期扫描获取系统的全局信息，做一些[统筹优化](#1.3 架构)工作。唯一缺点在于内存容量有限，但问题不大，因为一个 64MB 大小的块和单个文件都只有不到 64bits 的元数据，后者采用了前缀压缩技术存储文件名。

### 1.6.2 块位置

​	master 在启动时向块服务器查询每个块的副本位置，并在之后通过控制所有块的放置和定期发送心跳信息来获取更新块位置。块服务器可能发生各种变更如离开集群、损坏、重启等，保持 master 和块服务器的同步并非易事。而且块服务器对于某个块是否存在于其磁盘上具有最终话语权，在主服务器上保持一致信息没有意义。

​	**简言之，这种设计思想就是让每个块服务器负责检测本机器数据块的状态，并定期向 master汇报，一方面可以减少复杂的同步逻辑，简化系统设计，另一方面数据块的状态变更 master 无需插手。**

### 1.6.3 操作日志

​	操作日志包含了对重要的元数据变更的记录。有两方面的意义：

+ 用以持久化上述元信息，并且在 master 宕机恢复后进行元信息重建
+ 对于并发操作，确定其时间顺序。

​	采用**先写日志后写数据**的顺序，避免宕机后最近几次记录丢失。此外，日志也需要被复制到多台远程机器上，以**克服单点 master 的劣势**。为了减小频繁刷盘和复制对系统吞吐量的影响， master 会合并几条记录一次性写入磁盘。

​	为了避免每次都重头运行日志，master 结合**检查点**和日志重建系统状态。检查点以紧凑的 B 树形式存放在内存中，这种方式无需额外解析，加快了恢复速度。检查点的创建利用了类似于动态转储的技术，创建的同时不会阻塞正常的元数据变更。具体来说， master 会切换到一个新的日志文件并创建一个新的线程，检查点在旧线程中创建，包含切换前的一切变更。检查点同样也会远程机器上创建副本。

​	恢复只需要最新的完整检查点和随后的日志，老版本可以删除，但系统会保存几份以防万一。

## 1.7 一致性模型

​	GFS 采用了一种**宽松的一致性模型**，这种模型实现起来较为简单和高效。

### 1.7.1 GFS 的保证

​	在一次数据变更后，文件的状态取决于变更的类型、成功还是失败以及操作是否并发。下面解释两个概念：

+ **一致性**：同一个块在所有副本的数据都相同。
+ **已定义**：在满足上述条件的前提下，客户端能够完整地看到本次变更写入的数据。

​	下面讨论几种数据变更的场景：

+ 单用户写：变更区域是**已定义**的。
+ 并发写：变更区域处于**一致但未定义状态**，因为单个用户的修改可能被其他用户所覆盖。
+ 变更失败：变更区域处于**不一致状态**，如有的副本变更成功，有的副本变更失败。

+ 数据追加：在并发场景下，GFS 保证每个追加都会**被原子地至少执行一次**，操作的顺序由 GFS 决定，后者将追加位置的偏移量返回给用户。因此变更区域是**已定义的**。由于失败的追加操作可能造成记录之间出现填充或重复记录，这部分区域处于**不一致状态**。

​	在一系列成功的变更后，变更的文件区域处于已定义状态且包含最后一次变更写下的数据， GFS 通过在所有副本上执行同样的变更顺序以及使用块版本号检测过期块（由于块服务器故障而错过变更的块）来保证这一点。

​	然而，客户端元数据缓存带来的信息**不一致**可能造成其读到某一个数据块过期的副本，但当对应的 Cache 条目过期或文件被重新打开后，信息会被同步。

​	此外，变更成功后，组件故障仍然可能损坏数据， GFS 通过定期握手来识别发生故障的块服务器，通过检查和来检测数据是否损坏。

### 1.7.2 对应用的影响

​	GFS 应用通过以下几种技术来适应其宽松的一致性模型：

+ 数据追加和检查点。在一个典型的场景中，用户从头到尾生成一个文件。有两种方式保证一致性，一种是在写完所有数据后为文件永久重命名，另一种是阶段性地保存检查点。 reader 只读到最近一次的检查点，而每次创建检查点的文件一定处于已定义状态。总的来说，追加比随机写更加高效和鲁棒，而检查点技术允许 writer 阶段性重启，并防止 reader 读到不完整的数据。
+ 写入**自效验、自识别**的记录。在另一种典型的应用场景中，许多 writer 同时向一个文件追加以合并结果或者充当生产消费队列。为了处理因追加失败而导致的填充和重复记录， reader 使用检查和来识别并丢弃填充和记录碎片，使用记录中的唯一标识符过滤重复记录。这些功能以库函数的形式提供。

# 2. 系统交互

​	系统设计的目的是**最小化 master 在所有操作中的参与。**

## 2.1  租约和变更顺序

​	租约的目的是在保证所有副本变更顺序一致的同时，**减少 master 对并发操作的管理开销**。具体来说， master 将租约授予一个叫 primary 的副本，告诉它接下来将由它负责指定各变更的顺序，并命令其他副本按同样顺序执行。

​	租约拥有60s的初始时限。 primary 可通过心跳消息向 master 申请拓展。 master 也可以通过收回租约来阻止用户对某一个文件的修改。

![写操作的控制流和数据流](images\写操作的控制流和数据流.png)

​	写入数据的流程如图所示。通过**解耦数据流和控制流**，我们可以根据网络的拓扑结构灵活调度数据流，从而提高系统性能。

​	注意，图中只展示了一个客户端和 master 以及块服务器的交互，实际上可能同时存在多个客户端同时发起写请求， primary 通过4接受所有这些请求并为其安排连续的序列号。

​	数据写入的过程中，在任何副本上发生的任何错误都会被报告给客户端，本次请求被视为失败，此时客户端会重试步骤3-7。

​	如果应用程序的写入操作很大或跨越了块边界， GFS 客户端代码会将其分解为多个写入操作。

## 2.2 数据流

​	和控制信息先到从 primary 后到各次级服务器不同，数据沿着**精心挑选的块服务器链**以**流水线**方式**线性**推进。下面围绕三个设计目标展开阐述：

（1）**充分利用每一台机器的网络带宽**。数据采用线性的方式传输，而不是采用其他结构如树型，这样每台机器的输出带宽可以全部用来传输数据。如果采用树型结构，可能会导致某些中央节点负载太重，而叶子结点的带宽又没有得到充分利用。

（2）**避免网络瓶颈和高延迟连接**。每台机器都把数据发送给网络拓扑中距离“最近”的还未收到数据的节点。

（3）**最小化数据传输延迟**。 GFS 使用带流水线的持续 TCP 连接传输数据，当一个块服务器收到一部分数据后，它会立刻开始转发。全双工通信允许数据同时在两个方向上传输，因此数据转发并不会影响接收的速率。

## 2.3 原子记录追加

​	GFS 提供了一种名为 record append 的原子追加操作。和传统的写操作不同，客户端只需要指定追加的数据， GFS 将其原子地（即作为一个连续的字节序列）至少一次追加到文件中，偏移位置由 GFS 选择，并返回给客户端。如果 GFS 只支持传统的写入操作，应用程序需要采用某些复杂的同步技术来保证并发操作的原子性，如分布式锁管理器。

​	追加操作的流程只需要在[图2](#2.1  租约和变更顺序)的基础上增加一部分 primary 上的逻辑。在客户端向 primary 发送请求后， primary首先检查记录追加到当前块是否会使其超过 64MB 的最大尺寸。若超过，则将该块填充到最大尺寸，并回复客户端该操作应当被转移到下一个数据块上执行；否则正常追加记录。

​	如果记录在任何一个副本上追加失败，客户端会重新执行该操作。到最后 primary 回复客户端操作成功时，数据应当在所有副本的同一偏移位置追加（文件的这一部分区域被视为已定义）。此后，新的记录将在更高的偏移处追加，也就是说，**追加失败的那部分区域被跳过了**（文件的这一部分区域被视为不一致）。

## 2.4 快照

​	快照操作几乎可以即时创建文件或目录树的副本，同时最小化对正在发生的变更的影响。

​	GFS 使用**写时复制技术**来实现快照操作。当 master 收到快照请求时，它首先回收需要被创建快照的文件的数据块上的租约，暂时冻结这些文件的写入操作。随后， master 将该操作记录到磁盘，复制源文件或目录树的元数据（还是**先写日志后执行变更**）。此时，这些拷贝后的元信息逻辑上变成新文件， 但指向和原文件相同的块。

​	后续客户端写入这些数据块时，首先会向 master 发生请求以获取租约持有者， master 注意到该块的引用计数超过1，于是创建新的块句柄并要求持有该块副本的各个块服务器在**本地**完成复制。此后的操作和常规写入没有差别。

# 3. master 操作

## 3.1 命名空间管理和上锁

​	上锁目的：允许多个 master 操作并发执行，同时保证互斥操作间互不影响。当多个操作作用于不同文件区域时，可以并行；当作用于同一文件区域时，需要通过锁来保持互斥。

​	和传统的文件系统不同， GFS 的命名空间以**查找表**的形式保存在内存中，**该表将文件的路径名映射到其元数据**。命名空间树中的每一个节点（绝对文件名或绝对目录名）都有一个与之相关的读写锁。

​	每当涉及到命名空间的更改时（增删、重命名、快照等）， master 都会沿着目录树逐级获取一系列目录的读锁，直到目标文件的上一级目录为止，同时根据操作的类型获取目标文件的读/写锁。值得注意的是，创建文件并**不需要获取器父目录的写锁**，因为 GFS 没有目录或 inode 类似的数据结构，无需写锁来保证互斥修改，只需要获取读锁来防止父目录被删除就够了。

​	这种上锁机制**允许同一目录下的并发变更**，如多个文件可以在同一目录下同时被创建，因为同时加在其上级目录的读锁并不互斥。加在上级目录的读锁可以防止其**被删除、重命名或做快照**，而加在文件上的写锁确保创建同名文件的操作串行化。

​	由于命名空间拥有大量的节点，读写锁采用延迟分配策略且用完后立即删除。为了防止死锁，锁的分配遵循**一致的顺序**：先按照命名空间树内的层级排序，同一层内按照字典序排序。

## 3.2 副本放置

​	在 GFS 中，数据的分布是**多层级**的。数百台块服务器分布在不同的机架上，两台位于不同机架的机器之间的通信可能需要跨越一台或多台网络交换机。此外，机架间的网络带宽可能小于机架内所有机器的网络带宽之和。

​	块副本放置策略用作两个目的：最大化数据可靠性和可用性和最大化网络带宽利用率。因此，除了将副本放置在不同的机器上外，我们还需要**将数据副本分散到不同的机架中**，一方面可以应对整个机架损坏或离线导致副本不可用的情况，另一方面读操作可以利用多个机架的总带宽。与此同时，写流量也需要跨越多台机架，这是我们做出的权衡。

## 3.3 副本的创建、补齐和平衡

​	当块副本被创建时，有三种情况：数据块初始创建、副本补齐以及重新平衡。

​	首先当 master 初始创建一个块时，副本放置的位置需要遵循以下三个原则：

+ 优先考虑磁盘利用率低于平均水平的块服务器，即补齐短板。
+ 限制每台服务器最近创建的副本数量，以避免可能到来的大量写入流量。
+ 尽可能将副本分散在不同机架上。

​	其次，当可用的副本数量降低到用户设定值以下后， master 会启动补齐机制。 GFS 中待补齐的块可能有很多，我们需要为其安排一个优先级，主要考虑以下几点因素：

+ 和目标值的差距。差距越大优先级越高。
+ 所属文件的活跃状态。活文件的块优先补齐。
+ 副本的缺失是否阻塞用户进程。优先补齐这些瓶颈块。

​	master 每次选择拥有最高优先级的块进行副本补齐，它会命令那些持有该副本的块服务器复制该数据块，放置策略同[3.2](#3.2 副本放置)节。为了不影响客户端的正常访问， master 会在活跃的复制操作数目上对每个块服务器以及整个集群进行限制，同时也在带宽上做了限制。

​	最后， master 会定期检查当前副本的分布状况并通过移动副本来**充分利用硬盘空间并进行负载均衡**。在这个过程中， master 会逐渐填满一个块服务器而不是短时间填入大量新块。

## 3.4 垃圾回收

​	GFS 在文件和块层级上都采用延迟回收策略，即先删除（元数据），后回收（数据块）。

###3.4.1 回收机制

​	一个文件从被用户删除到数据块被回收，经过以下4步：

（1）**应用程序的删除**。当文件被应用删除后， master 将该操作写入日志，这时文件**并不会被立即删除**，而是被重命名到一个包含删除时间戳的隐藏名称，三天以内可以随时被用户通过重命名来恢复。

（2）**master 上文件元数据的删除**。被应用程序“删除”3天的隐文件会在master 在定期扫描文件系统的命名空间时被正式删除，此时该隐文件会被从命名空间中移除，其对应的元数据也会被从内存中删除（元数据1），这有效切断了文件到其数据块的链接（元数据2）。此时**数据块仍然保存在块服务器上**。

（3）**master 上块元数据的删除**。在 master 对数据块命名空间的扫描过程中，它识别出那些引用计数为0的孤儿块，并删除其元数据。

（4）**块服务器上块数据的删除**。块服务器通过心跳信息向 master 报告它拥有的所有数据块， master 通过在其命名空间中查找，告知它哪些数据块已被删除，块服务器可自由删除这些块。

### 3.4.2 进一步讨论

​	不同于传统的分布式垃圾回收，我们的设计方案十分简单。我们将保存所有文件和块的元数据的 master 作为一个中间桥梁，**任何不为 master 所知的副本都被视为垃圾**。

​	相较于立刻删除，我们的垃圾回收策略具有如下几个优点：

+ **在不可靠的场景中，垃圾回收更加简单可靠**。在组件故障频发的场景下，数据块创建可能在某些服务器上失败，副本删除信息也可能在传输过程中丢失，**垃圾回收策略提供一种统一回收上述出错遗留的垃圾的方法**，从而将 master 从处理各种操作出错的复杂逻辑中解脱出来。
+ **垃圾回收将分散的删除操作转变为定期集中清理**。将存储回收与主服务器的常规后台活动合并，**批量回收**的效率更高，还可以**和客户端请求错峰进行**。

+ **延迟回收的方式可以防止意外和不可逆转的删除**。

​	这种策略也会妨碍用户通过删除文件的方式缓解空间不足的情况，同时大量临时文件的创建与删除也会导致一部分空间被占用。解决方式包括通过重复删除加快空间回收、允许用户个性化定制复制和回收策略等。

## 3.5 过期副本删除

​	当一个块服务器因为故障而未及时对副本进行更新时，副本可能会过期。 **master 通过维护一个块版本号来区分过期和最新的副本**。

​	master 在授予一个 primary 租约时，会增加块版本号的数值并通知所有最新副本进行更新。对于最新的副本， master 与其一同更新版本号并将其持久化。若某个副本现在不可用，其版本号就不会得到更新。随后，当块服务器重启并向 master 报告其持有的数据块及其版本号时，master 会检测到这个过期的副本。

​	过期的副本会在垃圾回收阶段被 master 移除，在此之前， master 不会告知客户端该副本的存在。同时，客户端和块服务器在访问其他的块服务器时，也会验证版本号以确保访问到的是最新的数据。

# 4. 容错和诊断

​	组件故障会导致系统不可用和数据损坏， GFS 设计了一些机制来应对这些挑战，并构建了一些工具用于在错误发生后进行故障诊断。

## 4.1 高可用性

### 4.1.1 快速恢复

​	master 和块服务器能在几秒钟后恢复其状态并启动，无论以何种方式终止。

### 4.1.2 块复制

​	每个数据块都会被复制到不同机架上的多个块服务器上。若某些块意外过期或损坏， master 会及时进行副本补全。此外，研究人员也在探索其他的跨机器冗余方案，如奇偶校验或纠删码，以应对日益增长的只读存储需求。

### 4.1.3 master 复制

​	为了提升单点 master 的可靠性，我们将其操作日志和检查点**复制到多台远程机器上**。为了简化系统设计， master 上的单个进程负责管理所有数据变更以及后台活动，当它发生故障后，可以立即重启。如果是机器或者磁盘出现故障，GFS 外的监控设施会在别处启动一个新的 master 进程并使用操作日志的副本进行状态恢复。

​	GFS 还设置了一个叫做影子 master 的服务器用于提供对文件系统的只读访问，以防 master 出现故障。在元数据上影子 master 可能比 master 略有延迟。影子 master 通过读取操作日志来和 master 执行同步数据变更，并通过和块服务器交互来获取数据块位置等状态信息。对于副本的位置更新，影子 master 依赖于 master 的决策。

## 4.2 数据完整性

​	**块服务器通过检查和来检测存储数据的损坏。**由于 GFS 的记录追加不保证副本之间的完全一致，每个块服务器必须通过维护校验和**独立验证其自身副本的完整性**。

​	一个数据块被分成 64KB 的基本块，每块都有对应的 32 位校验和。和其他元数据一样，校验和保存在内存中，并与日志一起持久存储，与用户数据分开。

​	对于读操作，块服务器会验证一系列覆盖读取范围的基本块的检查和。校验对读取性能的影响较小，原因有几点。由于大多数读取操作至少跨越几个数据块，因此只需读取并校验少量额外数据即可完成验证； GFS 客户端代码通过在校验块边界处对齐读取，进一步减少了这种开销；此外，由于检查和保存在内存中，查找和比较无需任何I/O操作，且校验计算通常可以与I/O操作重叠进行。

​	对于追加操作，GFS 对其校验进行了较大的优化。具体来说，我们只对增加的数据计算校验和。这样，即使最后一个部分块损坏，也能通过比对其校验和发现。

​	对于覆写操作，覆盖范围内的所有块都需要重新计算校验和，但在写入之前我们需要先对头尾两个块进行验证，因为其中可能掺杂着旧数据。

​	对于长时间不被读取的数据块，其损坏很难被发现。在空闲时段，块服务器可以检查这些不活跃的块是否发生损坏，以防止主服务器误认为其拥有充足的副本数目。

## 4.3 诊断工具

​	GFS 服务器生成的诊断日志记录了许多重要事件（如块服务器的启动和关闭）以及所有 RPC 请求和响应。GFS 会在空间允许的情况下尽可能保留日志，以进行调试和性能分析。

​	RPC 日志包括在通信链路上发送的精确请求和响应，通过整理不同机器上的 RPC 记录，我们可以重建整个交互历史，以诊断问题。

​	日志记录对性能的影响极小，因为这些日志是按顺序和异步方式写入的。